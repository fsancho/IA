; N-Layer Neural Network matricial implementation in NetLogo 6 by Fernando Sancho
;   (based on a work of Stephen Larroque)
; 
; ============ Main Procedures =====================
;
; ANN:Train -- Main procedure to train Neural Network
;    Input: [MiniBatch Ws Architecture lambda Xtrain Ytrain Xtest Ytest  epsilon nIterMax debug?]
;           MiniBatch     : Size of minibatch to be used in every epoch 
;           Ws            : Initial Weight matrices (depends on Architecture)
;           Architecture  : Architecture of Network [N1 ... Nn]
;           lambda        : Regularization factor (0)
;           Xtrain        : Matrix with input data for Train Data
;           Ytrain        : Matrix with output data for Train Data
;           Xtest         : Matrix with input data for Test Data
;           Ytest         : Matrix with output data for Test Data
;           epsilon       : Learning rate for Back-propagation step
;           nIterMax      : Number of epochs for training loop
;           debug?        : Print info while training
;    Output: [Ws errtrain errtest]
;           Ws            : Weight Matrices after training
;           errtrain      : List of errors on train data
;           errtest       : List of errors on test data
;
; ANN:ForwardProp -- Evaluation/Forward Propagation of a Neural Network
;    Input: [Ws X]
;           Ws           : Weight Matrices
;           X            : Input data matrix to compute
;    Output: [O As Zs]
;           O            : Output data matrix
;           As           : List of matrix activations in every layer
;           Zs           : List of matrix 
;
; ANN:ComputeError -- Compute error from outputs
;    Input: [Y O Ws lambda]
;           Y            : Real/expected output data matrix
;           O            : Output data matrix from the network
;           Ws           : Weights of the network
;           lambda       : Regularization factor
;    Output: [err]
;           err          : mean error (number) of O from Y
;
; ANN:save-model -- Save Model Weights to a file
;    Input: [f Ws]
;           f            : File name
;           Ws           : Weights to save
;    Output: none
;
; ANN:load-model -- Load Model Weights from a file
;    Input: [f]
;           f            : File name
;    Output: [Ws]
;           Ws           : eight Matrices

extensions [matrix]

;==============================================================
;===========  NEURAL NETWORK FUNCTIONS  FILES =================
;==============================================================

to ANN:save-model [f Ws]
  file-open f
  file-print map matrix:to-row-list Ws
  file-close
end

to-report ANN:load-model [f]
  file-open f
  let rep map matrix:from-row-list (read-from-string file-read-line)
  file-close
  report rep
end


;==============================================================
;=================  NEURAL NETWORK FUNCTIONS  =================
;==============================================================

; Add a column of 1's at the beginning
to-report ANN:addBias [A]
  let dim matrix:dimensions A
  let N item 0 dim
  report matrix:prepend-column A (n-values N [1])
end

; Activation functions: pointwise and matricial forms
to-report ANN:f-act [x]
  report ANN:relu x
end

to-report ANN:f-act' [x]
  report ANN:relu' x
end

to-report ANN:m-act [A]
  report matrix:map ANN:f-act A
end

to-report ANN:m-act' [A]
  report matrix:map ANN:f-act' A
end


; RELU function
to-report ANN:relu [x]
  ifelse x > 0 [report x] [report 0]
end

to-report ANN:relu' [x]
  ifelse x > 0 [report 1] [report 0]
end

; Sigmoid function
;to-report ANN:sigmoid [x]
;  let ret 0
;  carefully [
;    set ret 1 / (1 + e ^ (- x)) ; use e ^ (-x) so as to show to user that some values are too big and may produce weird results in the learning
;  ]
;  [
;    show (word "Number too big for NetLogo, produces infinity in sig_aux with e ^ (- x): -x = " (- x))
;    set ret 1 / (1 + exp (- x)) ; prefer using (exp x) rather than (e ^ x) because the latter can produce an error (number is too big) while exp will produce Infinity without error
;  ]
;  report ret
;end

to-report ANN:sigmoid [x]
  report 1 / (1 + exp (- x))
end

to-report ANN:sigmoid' [x]
  let s ANN:sigmoid x
  report s * (1 - s)
end


; Initialize all the weights matrices
to-report ANN:InitializeWs [Architecture]
  let Ws (map [[i j] -> (ANN:RandInitializeW i j)] (bl Architecture) (bf Architecture))
  report Ws
end

; Rand initilization of one weight matrix
to-report ANN:RandInitializeW [L_in L_out]
  let W matrix:make-constant L_out (L_in + 1) 0
  let epsilon_init ((sqrt 6) / (sqrt (L_in + L_out))) ; or 0.12, both are magic values anyway, so you can change that to anything you want
  set W matrix:map [((random-float 1) * 2 * epsilon_init - epsilon_init)] W
  report W
end

to-report ANN:Train [MiniBatch Ws Architecture lambda Xtrain Ytrain Xtest Ytest  epsilon nIterMax debug?]

  let dim matrix:dimensions Xtrain
  let M item 0 dim

  let Layers (length Architecture)

  let t 1
  let errtrain []
  let errtest []

  let timer0 timer
  let porc 0
  while [t < nIterMax] [
    ; == Preparing for stochastic gradient (pick only one example for each iteration instead of all examples at once)
    let samples n-of MiniBatch (range M)
    let XBatch select-samples samples Xtrain
    let YBatch select-samples samples Ytrain
    
    ; == Forward propagating + Computing cost
    ; = Learning dataset
    let forward-train (ANN:ForwardProp Ws XBatch)
    let Out (item 0 forward-train)
    let As (item 1 forward-train)
    let Zs (item 2 forward-train)
    
    if debug? [
      ; == Train and Test error
      set errtrain lput (ANN:ComputeError YBatch Out Ws lambda) errtrain
      
      let forward-test (ANN:ForwardProp Ws Xtest)
      let Otest (item 0 forward-test)
      let Atest (item 1 forward-test)
      let Ztest (item 2 forward-test)
      
      set errtest lput (ANN:ComputeError Ytest Otest Ws lambda) errtest
    ]
    
    ; == Back propagating
    ; Computing gradient
    let Ws_grad (ANN:BackwardProp lambda Ws As Zs YBatch)
    ; Update Ws
    let epsi (epsilon / (1 + (t - 1) * 0)) ; Implementing learning rate decay (aka gradient step decay) so that the network converges faster
    set Ws (map [[W W'] -> (W matrix:- (epsi matrix:* W'))] Ws (bl Ws_grad))

    ; Increment t (iteration counter)
    set t (t + 1)
    
    if debug? [
      let pas floor (100 * (t / nIterMax))
      if (pas mod 10 = 0) and  (pas > porc) [set porc porc + 10 print (word pas "% (" (timer - timer0)")")]
    ]
  ]
  report (list Ws errtrain errtest)
end

to-report select-samples [samples X]
  report matrix:from-row-list (map [i -> matrix:get-row X i] samples)
end

; Forward Propagation of ANN
to-report ANN:ForwardProp [Ws X]
  let Layers (length Ws) + 1
  let As (list ANN:addBias(X))
  let Zs [0]
  let Z 0
  foreach (range 1 Layers) [
    L ->
    let A last As
    let W (item (L - 1) Ws)
    set Z A matrix:* (matrix:transpose W)
    set Zs lput Z Zs
    set As lput (ANN:addBias (ANN:m-act Z)) As
  ]
  set As lput Z (bl As) ; Change the last activation for the same but without sigmoid
  let O Z
  report (list O As Zs)
end

to-report ANN:ComputeError [Y O Ws lambda]

  ; First, the Predictive error: (O - Y)^2
  if is-number? Y [ set Y matrix:from-row-list (list (list Y)) ] ; Convert to a matrix if it's only a number
  if is-number? O [ set O matrix:from-row-list (list (list O)) ] ; Convert to a matrix if it's only a number
  let Y-O (Y matrix:- O)
  let J1 sum matrix:to-list (matrix:times-element-wise Y-O Y-O) ; First we must sum over the features to compute some kind of euclidian distance,
                                                                   ; then we can sum over all examples errors
  let M item 0 matrix:dimensions Y
  set J1 (1 / (2 * M) * J1) ; scale cost relatively to the size of the dataset

  ; Second, the Regularization factor (if lambda > 0, then small values in Ws are choosen)
  let Ws_red map [th -> matrix:bfc th] Ws
  let sum_Ws sum (map [ th -> sum matrix:to-list (matrix:times-element-wise th th)] Ws_red)
  let J2 lambda / (2 * M) * sum_Ws

  ; Sum
  let J (J1 + J2)

  report J
end

to-report ANN:BackwardProp [lambda Ws As Zs Y]
  let Layers length As
  let M item 0 matrix:dimensions Y

  let deltas (n-values Layers [0])
  let O (item (Layers - 1) As)

  if is-number? Y [ set Y matrix:from-row-list (list (list Y)) ] ; Convert to a matrix if it's only a number

  ; == backpropagate delta values
  set deltas (replace-item (Layers - 1) deltas (O matrix:- Y))
  foreach (range (Layers - 2) 0 -1) [
    L ->
    let W_L item L Ws
    let Z_L item L Zs
    let delta_L+1 item (L + 1) deltas
    let delta_L (delta_L+1 matrix:* (matrix:bfc W_L))
    set delta_L (matrix:times-element-wise delta_L (ANN:m-act' Z_L))
    set deltas (replace-item L deltas delta_L)
  ]

  ; == Backpropagating error gradient (for the weights) from last layer towards first layer
  let Ws_grad (n-values Layers [0])
  foreach (range (Layers - 2) -1 -1) [
    L ->
    let A item L As
    let delta_L+1 item (L + 1) deltas
    let W_L_grad (matrix:times (1 / M) (matrix:transpose delta_L+1) A)
    set Ws_grad (replace-item L Ws_grad W_L_grad)
  ]

  ; == Regularize the gradient
  foreach (range (Layers - 2) -1 -1) [
    L ->
    let W_L_grad (item L Ws_grad)
    let Wn_L_grad matrix:bfc W_L_grad
    let W1_L_grad (matrix:get-column W_L_grad 0)
    let W_L (item L Ws)
    set W_L_grad Wn_L_grad matrix:+ ((lambda / M)  matrix:* (matrix:bfc W_L))
    set Ws_grad (replace-item L Ws_grad (matrix:prepend-column W_L_grad W1_L_grad))
  ]

  report Ws_grad
end

;==============================================================
;================= MATRIX AUXILIARY FUNCTIONS =================
;==============================================================

; row is a list
to-report matrix:prepend-row [X row]
  report matrix:from-row-list fput row (matrix:to-row-list X)
end

; column is a list
to-report matrix:prepend-column [X column]
  report matrix:from-column-list fput column (matrix:to-column-list X)
end

; Semi-vectorized procedure to sum a matrix over rows or columns
; If the matrix is not summable or there is only one element, it will return the same matrix
to-report matrix:sum [X columnsOrRows?] ; columnsOrRows? 2 = over columns
  if is-number? X [report X] ; if it's already a number, we've got nothing to do, just return it
  if is-list? X [report sum X] ; if it's a list we use the built-in function
  if columnsOrRows? = 2 [ report map sum matrix:to-column-list X ] ; Columns
  if columnsOrRows? = 1 [ report map sum matrix:to-row-list X ] ; Rows
end

; Semi-vectorized procedure to return the max value of a matrix over rows or columns
; If the matrix is not summable or there is only one element, it will return the same matrix
; NB: it's a copy-cat of matrix:sum
to-report matrix:max [X columnsOrRows?] ; columnsOrRows? 2 = over columns
  if is-number? X [report X] ; if it's already a number, we've got nothing to do, just return it
  if is-list? X [report max X] ; if it's a list we use the built-in function
  if columnsOrRows? = 2 [ report map max matrix:to-column-list X ] ; Columns
  if columnsOrRows? = 1 [ report map max matrix:to-row-list X ] ; Rows
end

; Semi-vectorized procedure to return the min value of a matrix over rows or columns
; If the matrix is not summable or there is only one element, it will return the same matrix
; NB: it's a copy-cat of matrix:sum
to-report matrix:min [X columnsOrRows?] ; columnsOrRows? 2 = over columns
  if is-number? X [report X] ; if it's already a number, we've got nothing to do, just return it
  if is-list? X [report min X] ; if it's a list we use the built-in function
  if columnsOrRows? = 2 [ report map min matrix:to-column-list X ] ; Columns
  if columnsOrRows? = 1 [ report map min matrix:to-row-list X ] ; Rows
end

; Deep recursive copy of a megamatrix (list of matrix)
to-report matrix:copy-recursive [megamatrix]
  ; Entry point: if it's a list, we will recursively copy everything inside (deep copy)
  ifelse is-list? megamatrix [
    let n (length megamatrix)
    let m2 (n-values n [0])

    let i 0
    while [i < n] [
      set m2 (replace-item i m2 (matrix:copy-recursive (item i megamatrix)))
      set i (i + 1)
    ]
    report m2
  ]
  ; Recursively called from here
  [
    ; If it's a number or string we just report it
    ifelse is-number? megamatrix or is-string? megamatrix [
      report  megamatrix
    ]
    ; Else if it's a matrix (we have no other way to check it but by default), then we make a copy and report it
    [
      report matrix:copy megamatrix
    ]
  ]
end

; Devuelve la misma matriz, pero eliminando la primera columna
to-report matrix:bfc [X]
  let dim matrix:dimensions X
  let N first dim
  let M last dim
  report matrix:submatrix X 0 1 N M
end

; Devuelve una matriz con una sola fila de i (pero es matriz)
to-report matrix:1row [X i]
  report matrix:from-row-list (list matrix:get-row X i)
end

to-report matrix:somerows [X indx]
  let Xlist map [i -> matrix:get-row X i] indx
  report matrix:from-row-list Xlist
end

to-report matrix:to-list [X]
  report reduce sentence matrix:to-row-list X
end